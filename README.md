# Study-PyToch
## day 1
### PyToch
파이토치(PyToch)는 딥러닝 모델을 구축하고 훈련시키는 데 사용되는 오픈 소스 딥러닝 프레임워크입니다.
 1. 동적 그래프(Dynamic Computational Graph) :
    - 파이토치는 동적 그래프를 사용하여 딥러닝 모델을 정의하고 실행한다. 이는 계산 그래프가 런타임 중에 정의되고 변경될 수 있다는 것을 의미한다. 이러한 접근 방식은 모델의 작성과 디버깅을 단순화하고, 유연성을 제공한다.
 2. 모듈화된 구성요소 :
    - 파이토치에서는 다양한 모듈화된 구성요소를 제공하여 딥러닝 모델을 구축한다.
 3. 텐서(Tensor) : 
    - 파이토치에서는 텐서(tensor)라고 불리는 다차원 배열을 기본 데이터 구조로 사용한다. 텐서는 넘파이(NumPy) 배열과 유사하지만, GPU를 사용하여 연산을 가속화할 수 있다.
 4. 자동 미분(Automatic Differentiation) :
    - 파이토치는 자동 미분을 지원하여 역전파(backpropagation)를 쉽게 수행할 수 있다. 이는 딥러닝 모델을 훈련시킬 때 그래디언트를 자동으로 계산하여 모델의 매개변수를 최적화하는 데 도움을 준다.
 5. 신경망 모듈(Neural Network Module) :
    - 파이토치는 신경망 모듈이라는 모듈화된 구성 요소를 제공하여 딥러닝 모델을 쉽게 구축할 수 있다. 이 모듈은 레이어(layer), 활성화 함수(activation function), 손실함수(loss function)등 다양한 요소로 구성된다.
 6. GPU 가속 :
    - 파이토치는 CUDA를 사용하여 GPU 가속을 지원한다. 따라서 대규모 데이터셋과 복잡한 모델을 효율적으로 처리할 수 있다.
 7. 편리한 API :
    - 파이토치는 사용자 친화적인 API를 제공하여 빠르고 쉽게 딥러닝 모델을 구축하고 훈련시킬 수 있다. 이는 코드를 간결하게 작성하소 이해하기 쉽게 만들어준다.
 8. 확장성 :
    - 파이토치는 모듈화된 디자인을 통해 확장성이 높다. 이는 사용자가 필요에 따라 커스텀한 모델과 기능을 쉽게 추가학소 확장할 수 있다.
 9. 편리한 데이터 로딩 및 전처리 :
    - "torchvision", "torchtext" 등의 라이브러리를 통해 이미지, 텍스트 등 다양한 유형의 데이터를 쉽게 로드하고 전처리할 수 있다.
     
### 데이터 작업하기
파이토치(PyToch)에는 데이터 작업을 위한 기본 요소 두가지인 torch.utils.data.DataLoader 와 torch.utils.data.Dataset가 있다.
   - Dataset : 샘플과 정답(label)을 저장
   - DataLoader : Dataset을 순회 가장한 객체(iterable)로 감싼다.
torchvision.datasets 모듈은 CIFAR, COCO 등과 같은 다양한 실제 비전(vision) 데이터에 대한 Dataset을 포함하고 있다. 모든 TorchVision Dataset은 샘플과 정답을 각각 변경하기 위한 transform 과 target_transform의 두 인자를 포함한다.  
Dataset을 DataLoader의 인자로 전달한다. 이는 데이터셋을 순회 가능한 객체(iterble)로 감싸고, 자동화된 배치(batch), 샘플링(sampling), 섞기(shuffle) 및 다중 프로세스로 데이터 불러오기를 지원한다. 여기서는 배치 크기를 64로 정의한다. 즉, 데이터로더 객체의 각 요소는 64개의 특징과 정답을 묶음으로 반환한다.  
  
### 모델 만들기
PyTorch에서 신경망 모델은 nn.Module을 상속받는 클래스(class)를 생성하여 정의한다. __init__함수에서 신경망의 계층(layer)들을 정의하고 forward함수에서 신경망에 데이터를 어떻게 전달하지 지정한다. 가능한 경우 GPU 또는 MPS로 신경망을 이동시켜 연산을 가속한다.
#### 신경망 모델 구성하기
신경망은 데이터에 대한 연산을 수행하는 계층(layer)로 구성되어 있다. torch.nn 네임스페이스는 신경망을 구성하는데 필요한 모든 구성 요소를 제공한다. Pytorch의 모든 모듈은 nn.Module의 하위 클래스(subclass) 이다. 신경망은 다은 모듈(계층; layer)로 구성된 모듈이다. 이러한 중첩된 구조는 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있다.
#### 학습을 위한 장치 얻기
가능한 경우 GPU 또는 MPS와 같은 하드웨어 가속기에서 모델을 학습하려고 합니다. torch.cuda 또는 torch.backends.mps 가 사용 가능한지 확인해보고, 그렇지 않으면 CPU를 계속 사용합니다.  
#### 클래스 정의하기
신경망 모델을 nn.Module 의 하위클래스로 정의하고, __init__ 에서 신경망 계층들을 초기화합니다. nn.Module 을 상속받은 모든 클래스는 forward 메소드에 입력 데이터에 대한 연산들을 구현합니다.  
NeuralNetwork 의 인스턴스(instance)를 생성하고 이를 device 로 이동한 뒤, 구조(structure)를 출력합니다.  
  
*** 모델을 사용하기 위해 입력 데이터를 전달합니다. 이는 일부 백그라운드 연산들 과 함께 모델의 forward 를 실행합니다. model.forward() 를 직접 호출하지 마세요! ***
  
모델에 입력을 전달하여 호출하면 2차원 텐서를 반환합니다. 2차원 텐서의 dim=0은 각 분류(class)에 대한 원시(raw) 예측값 10개가, dim=1에는 각 출력의 개별 값들이 해당합니다. 원시 예측값을 nn.Softmax 모듈의 인스턴스에 통과시켜 예측 확률을 얻습니다.  
  
#### 모델 계층(Layer)
FashionMNIST 모델의 계층들을 살펴보겠습니다. 이를 설명하기 위해, 28x28 크기의 이미지 3개로 구성된 미니배치를 가져와, 신경망을 통과할 때 어떤 일이 발생하는지 알아보겠습니다.  
##### nn.Flatten
nn.Flatten 계층을 초기화하여 각 28x28의 2D 이미지를 784 픽셀 값을 갖는 연속된 배열로 변환합니다. (dim=0의 미니배치 차원은 유지됩니다.)
##### nn.Linear
선형 계층 은 저장된 가중치(weight)와 편향(bias)을 사용하여 입력에 선형 변환(linear transformation)을 적용하는 모듈입니다.
##### nn.ReLU
비선형 활성화(activation)는 모델의 입력과 출력 사이에 복잡한 관계(mapping)를 만듭니다. 비선형 활성화는 선형 변환 후에 적용되어 비선형성(nonlinearity) 을 도입하고, 신경망이 다양한 현상을 학습할 수 있도록 돕습니다.  
이 모델에서는 nn.ReLU 를 선형 계층들 사이에 사용하지만, 모델을 만들 때는 비선형성을 가진 다른 활성화를 도입할 수도 있습니다.  
##### nn.Sequential
nn.Sequential 은 순서를 갖는 모듈의 컨테이너입니다. 데이터는 정의된 것과 같은 순서로 모든 모듈들을 통해 전달됩니다. 순차 컨테이너(sequential container)를 사용하여 아래의 seq_modules 와 같은 신경망을 빠르게 만들 수 있습니다.  
##### nn.Softmax
신경망의 마지막 선형 계층은 nn.Softmax 모듈에 전달될 ([-infty, infty] 범위의 원시 값(raw value)인) logits 를 반환합니다. logits는 모델의 각 분류(class)에 대한 예측 확률을 나타내도록 [0, 1] 범위로 비례하여 조정(scale)됩니다. dim 매개변수는 값의 합이 1이 되는 차원을 나타냅니다.  
##### 모델 매개변수
신경망 내부의 많은 계층들은 매개변수화(parameterize) 됩니다. 즉, 학습 중에 최적화되는 가중치와 편향과 연관지어집니다. nn.Module 을 상속하면 모델 객체 내부의 모든 필드들이 자동으로 추적(track)되며, 모델의 parameters() 및 named_parameters() 메소드로 모든 매개변수에 접근할 수 있게 됩니다.

이 예제에서는 각 매개변수들을 순회하며(iterate), 매개변수의 크기와 값을 출력합니다.

### 모델 매개변수 최적화하기
모델을 학습하려면 손실 함수(loss function)와 옵티마이저(optimizer)가 필요하다.
